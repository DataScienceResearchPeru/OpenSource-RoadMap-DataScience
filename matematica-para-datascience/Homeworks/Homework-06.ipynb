{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Set Week 6\n",
    "## insert your name here\n",
    "\n",
    "In this homework assignment please fill the indicated cells with your code and explainations, ***run*** everything (select `cell` in the menu, and click `Run all`), save the notebook with your name appended to the filename (for example, `Homework-06-caos.ipynb`), and upload it to canvas.\n",
    "\n",
    "This homework assignment mainly concerns about regression and how to use gradient descent to find the weights. For this homework you need the following modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Gradient descent in $n$-dimension\n",
    "\n",
    "Consider the following minimization problem for $\\mathbf{x}\\in \\mathbb{R}^n$, for a symmetric positive semi-definite $n\\times n$ matrix $A$ \n",
    "$$\n",
    "\\min_{\\mathbf{x}\\in \\mathbb{R}^n} f(x) = \\min_{\\mathbf{x}\\in \\mathbb{R}^n} \\left\\{\\frac {1}{2} \\mathbf{x}^T A \\mathbf{x} - \\mathbf{b}^T \\mathbf{x},\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "A straightforward computation (which will be featured in Math 110) yields:\n",
    "\n",
    "$$\n",
    "\\nabla f (\\mathbf{x}) = A\\mathbf{x} -\\mathbf{b}.\n",
    "$$\n",
    "\n",
    "Hence the gradient descent algorithm reads:\n",
    "> Choose initial guess $\\mathbf{x}_0$ and learning rate $\\eta$<br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $\\mathbf{x}_{k+1} =  \\mathbf{x}_k - \\eta\\big( A\\mathbf{x}_k -\\mathbf{b}\\big)$\n",
    "\n",
    "which recovers the famous [Richardson iteration](https://en.wikipedia.org/wiki/Modified_Richardson_iteration) to solve a linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, our $A$ is a Laplacian matrix of a directed graph, and graph theory is widely used to explain the approximation capability of a neural network. Our $A$ can be generated by the following routine using the adjacency matrix $G$ (representing the connected edges between nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csgraph\n",
    "G = np.array([[0,  1,  0,  1,  1],  \n",
    "              [1,  0,  1,  1,  1], \n",
    "              [0,  1,  0,  1,  0], \n",
    "              [1,  1,  1,  0,  1], \n",
    "              [1,  1,  0,  1,  0]])\n",
    "A = csgraph.laplacian(G, normed=True)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question of problem 1:\n",
    "* (Graded) Apply the gradient descent algorithm to try to minimize $\\frac {1}{2} \\mathbf{x}^T A \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$ for the matrix $A$ above, for example, 200 iterations of vanilla gradient descent with step size 0.1, and $\\mathbf{b}$ is randomly generated by `np.random.normal()`. What have you observed?\n",
    "* Replace $A$ with $A + \\epsilon I$ where $I$ is an identity matrix, $\\epsilon=0.01$, and apply the gradient descent to the minimization problem again. What have you observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Overfitting\n",
    "\n",
    "Consider the following synthetic data set generated by $y=x^2$ with random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_samples = 50\n",
    "X = np.linspace(0,2,num_samples)\n",
    "y = X**2 + np.random.normal(0,0.5, size=num_samples)\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A customized class from scikit-learn\n",
    "\n",
    "Below are the code of a class called `PolyFitter` that uses the built-in linear regression model in `scikit-learn` to fit not just a linear function but a polynomial function of any degree, e.g. $f(x) = w_{10}x^{10} + w_9x^9 + \\dots + b$, to the data. \n",
    "\n",
    "\n",
    "The code constructs a fit to a general polynomial. Not just the linear moments, it considers the moments in $x^2$, $x^3$, $\\dots$ to the model as if they were extra information about the data points. \n",
    "\n",
    "Optional: Read the `_augment_input_()` function below and the [Vandermonde matrix formulation](https://en.wikipedia.org/wiki/Polynomial_regression#Matrix_form_and_calculation_of_estimates) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class PolyFitter():\n",
    "    \n",
    "    def __init__(self, degree=1):\n",
    "        self.degree = degree\n",
    "        self.model = LinearRegression() # initialize using the LinearRegression() class\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        X_aug = self._augment_input_(X)\n",
    "        self.model.fit(X_aug, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_aug = self._augment_input_(X)\n",
    "        return self.model.predict(X_aug)\n",
    "    \n",
    "    def _augment_input_(self, X): \n",
    "        # this function essentially add x^2, x^3, etc to the data points\n",
    "        N = X.shape[0]\n",
    "        k = self.degree\n",
    "        X_aug = np.zeros([N,k])\n",
    "        X_aug[:,0] = X[:]\n",
    "        for i in range(1,k):\n",
    "            X_aug[:, i] = X * X_aug[:, i-1]\n",
    "        return X_aug\n",
    "    \n",
    "    def plot(self, X, y):\n",
    "        XX = np.linspace(-2,5,600)\n",
    "        yy = self.predict(XX)\n",
    "        plt.scatter(X, y, s=100, alpha=0.4)\n",
    "        plt.axis('tight')\n",
    "        plt.plot(XX,yy,'red')\n",
    "        plt.show()\n",
    "        \n",
    "    def mean_squared_error(self,X,y):\n",
    "        # your code here (see instructions below)\n",
    "        # you should use the predict function above\n",
    "        # delete the pass line when submitting\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use `PolyFitter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolyFitter(degree=2)\n",
    "model.train(X, y)\n",
    "model.plot(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolyFitter(10) # a pure number input is fine as well\n",
    "model.train(X, y)\n",
    "model.plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "As you can see, picking a high degree polynomial fits the data much more precisely at certain points. But clearly, this high-degree approximation is not really capturing the essence of this dataset.\n",
    "\n",
    "Problem 2 contains the following question:\n",
    "\n",
    "* Write the `mean_squared_error(X, y)` function for the `PolyFitter` class above. It should return the average of the square of the difference between the model's predictions (obtained using `self.predict(X)`) and the given answers `y` (this is called \"ground truth\" in many machine learning literature). This will allow us to see how our model is doing.\n",
    "\n",
    "* Generate a new set of data with 1000 points using $y=x^2$ plus normal random noise like in the beginning. Split the data into 75% training examples and 25% test examples. You don't need to shuffe the data, just split it into two parts. If you want to shuffle the data, you can use the following [split routine from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html):\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "```\n",
    "Alternatively, you can manually make four numpy arrays: `X_train`, `y_train`, `X_test` and `y_test`  from `X` and `y`. The training arrays should contain the $x$ and $y$ coordinates of 75% of data-points while the test arrays should contain the remaining 25%. We will use the training samples to train the model, and the test pair to see how our model is doing on data that it has not seen before (the test data). This is a standard method for avoiding the mistake of making models that work really well during development, but then work very badly when they see data they have never seen before.\n",
    "    \n",
    "* For each degree $d = 1,2,\\dots,10$: train a `model = PolyFitter(degree=d)` with the data `X_train`, `y_train`; compare the fitted curve with the data using `model.plot()`.\n",
    "\n",
    "* Compute the *training errors* (the MSE for the `X_train`'s model prediction and `y_train`); and the *testing errors* (the MSE between `X_test`'s model prediction and `y_test`) for $d=1,\\dots,10$. \n",
    "\n",
    "* Plot the *training errors* and *test errors* computed above as a function of $d=1,\\dots,10$.  \n",
    "\n",
    "You should notice that as the degree increases, the training error goes down a lot, which means that our model is becoming very good at fitting to the training data. But, for the test data, it gets worse and worse after some point. What's the best degree for this data-set?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
