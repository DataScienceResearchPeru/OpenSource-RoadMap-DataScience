{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lecture we have learned how to set up and use the softmax regression to classify all 10 handwritten digits based on pixel intensities on a 28x28 grid (MNIST dataset).\n",
    "\n",
    "However, the training is slow on a decent configured laptop due to the size the dataset and the size of the parameter.\n",
    "\n",
    "Today we will learn a new method called Stochastic Gradient Descent (SGD), one of the two pillars of the deep learning (the other being backpropagation).\n",
    "\n",
    "> Every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize (stochastic) gradient descent.\n",
    "\n",
    "References:\n",
    "* [Why Momentum Really Works](https://distill.pub/2017/momentum/)\n",
    "* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent\n",
    "We consider a loss function (e.g., the one we saw in softmax regression) which is the average of the sample-wise loss:\n",
    "\n",
    "$$L(\\mathbf{w}) := L(\\mathbf{w}; X,\\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N f_i(\\mathbf{w}; \\mathbf{x}^{(i)},y^{(i)})$$ \n",
    "\n",
    "which has the weights $\\mathbf{w}$ as the parameters. Let us recall the softmax loss function here for comparison:\n",
    "\n",
    "$$\n",
    "L (\\mathbf{w})  = - \\frac{1}{N}\\sum_{i=1}^N \\Big\\{\\text{cross-entropy for each sample}  \\Big\\}=- \\frac{1}{N}\\sum_{i=1}^N \\left\\{\\sum_{k=1}^K\n",
    "1_{\\{y^{(i)} = k\\}} \\ln \\Bigg( \\frac{\\exp(\\mathbf{w}_k^{\\top} \\mathbf{x}^{(i)})}{\\sum_{j=1}^{K} \n",
    "\\exp\\big(\\mathbf{w}_j^{\\top} \\mathbf{x}^{(i)} \\big) }  \\Bigg)\\right\\}.\n",
    "$$\n",
    "\n",
    "Then the gradient descent method for it reads:\n",
    "\n",
    "> Choose initial guess $\\mathbf{w}_0$, step size (learning rate) $\\eta$, number of iterations $M$<br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{k+1} =  \\mathbf{w}_k - \\eta\\nabla_{\\mathbf{w}} L(\\mathbf{w}_k) =  \\mathbf{w}_k - \\eta\\frac{1}{N}\\sum_{i=1}^N \\nabla_{\\mathbf{w}} f_i(\\mathbf{w}; \\mathbf{x}^{(i)},y^{(i)})$\n",
    "\n",
    "The gradient has to be evaluated $N$ times in one iteration, each evaluation involves a matrix matrix multiplication of order $O(n)$ (number of features in one sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks of gradient descent: \n",
    "* Large amount of gradient evaluation (for each sample), long computation time, wasting electricity, etc. \n",
    "* Suppose we add more examples to our training set. For simplicity, imagine we just add an extra copy of every training example (but the computer algorithm does not know it is the same samples): then the amount of work doubles!\n",
    "$$\n",
    "\\nabla L = \\frac{1}{2N}\\sum_{i=1}^N \\nabla f_i(\\mathbf{w}) + \\frac{1}{2N}\\sum_{i=1}^N \\nabla f_i(\\mathbf{w}) ,\n",
    "$$\n",
    "even for the same loss function.\n",
    "* The training examples arrive one-at-a-time (or several-at-a-time) as the model is \"learning\" (through gradient descent to improve the accuracy). Should we include these into the original dataset and re-compute the gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Suppose our loss function is still:\n",
    "\n",
    "$$L := L(\\mathbf{w}; X,\\mathbf{y}) =  \\frac{1}{N}\\sum_{i=1}^N f_i(\\mathbf{w}; \\mathbf{x}^{(i)},y^{(i)}),$$\n",
    "\n",
    "where $X = (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$ are the training samples, $\\mathbf{y} = (y^{(1)}, \\dots, y^{(N)})^{\\top}$ are the labels/taget values for the training samples.\n",
    "\n",
    "> Choose initial guess $\\mathbf{w}_0$, step size (learning rate) $\\eta$, number of inner iterations $M$, number of epochs $n_E$ <br><br>\n",
    ">    Set $\\mathbf{w}_{M+1} = \\mathbf{w}_0$ for epoch $e=0$<br>\n",
    ">    For epoch $n=1,2, \\cdots, n_E$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w}_{0}$ for the current epoch is $\\mathbf{w}_{M+1}$ for the previous epoch.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the samples so that $\\{\\mathbf{x}^{(m)},y^{(m)}\\}_{m=1}^N$ is a permutation of the original dataset.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{m+1} = \\mathbf{w}_m - \\eta \\nabla f_i(\\mathbf{w}; \\mathbf{x}^{(m)},y^{(m)})$\n",
    "\n",
    "If $M = N$, which is the current batch of all training samples, one outer iteration is called a completed *epoch*.\n",
    "\n",
    "### Vanilla SGD: Single gradient evaluation at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Let us give it a go on the linear regression. This time, we are using `scikit-learn`'s built-in dataset generator. For the linear regression, we can use `scikit-learn`'s `LinearRegression()` class for the multivariate regression from previous lectures, but since we are illustrating SGD, we are implementing ourselves. \n",
    "\n",
    "Recall the loss function with the $L^2$ regularization and the gradient: let the weight $\\mathbf{w} = (w_0, \\widehat{\\mathbf{w}})$ where $w_0$ is the bias, and $\\widehat{\\mathbf{w}}$ is the vector containing the weights for the features of the dataset\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N  \n",
    "\\left( [1, \\;\\mathbf{x}^{(i)}]^{\\top} \\mathbf{w} - y^{(i)} \\right)^2 \n",
    "+ \\epsilon |\\widehat{\\mathbf{w}}|^2,\n",
    "\\\\\n",
    "\\frac{\\partial L(w)}{\\partial \\mathbf{w}} = \\frac{2}{N}\\sum_{i=1}^N [1, \\;\\mathbf{x}^{(i)}]\\left( [1, \\;\\mathbf{x}^{(i)}]^{\\top} \\mathbf{w} - y^{(i)}\\right) + 2\\epsilon\\, [0, \\widehat{\\mathbf{w}}]\n",
    "$$\n",
    "\n",
    "If there is no bias:\n",
    "$$\n",
    "L(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N  \n",
    "\\left( \\mathbf{w}^{\\top}\\mathbf{x}^{(i)}  - y^{(i)} \\right)^2 \n",
    "+ \\epsilon |\\mathbf{w}|^2,\n",
    "\\\\\n",
    "\\frac{\\partial L(w)}{\\partial \\mathbf{w}} = \\frac{2}{N}\\sum_{i=1}^N \\mathbf{x}^{(i)}\n",
    "\\left( \\mathbf{w}^{\\top}\\mathbf{x}^{(i)}  - y^{(i)}\\right) + 2\\epsilon\\,\\mathbf{w}.\n",
    "$$\n",
    "\n",
    "\n",
    "Reference: [Scikit-learn's dataset loading utilities](https://scikit-learn.org/stable/datasets/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=10000, n_features=10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-3 # regularization parameter\n",
    "# our model, returns the linear function [1 X]^T w\n",
    "def h(X, w):\n",
    "    # X is the training data\n",
    "    return np.matmul(X,w)\n",
    "\n",
    "# loss function = total square error on the given data set X,y\n",
    "def loss(w, X, y):\n",
    "    # N = len(y) \n",
    "    # this is one of the key, if y is only part of/one of the data label, then N will be small!\n",
    "    residual_components = h(X, w) - y\n",
    "    regularization = eps*np.sum(w**2)\n",
    "#     return (1/len(y))*np.sum(residual_components**2) + regularization\n",
    "    return np.mean(residual_components**2) + regularization\n",
    "    # the second implementation is prefered due to the len(y) may not exist\n",
    "\n",
    "def gradient_loss(w, X, y):\n",
    "    gradient_for_all_training_data = (h(X,w) - y).reshape(-1,1)*X\n",
    "    gradient_for_regularization = 2*eps*w\n",
    "    gradient_mean_training_data = np.mean(gradient_for_all_training_data, axis=0)\n",
    "    # we should return a (10,) array, which is averaging all training data\n",
    "    return 2*gradient_mean_training_data + gradient_for_regularization\n",
    "\n",
    "# we define a cross validating function to compute the R^2 score \n",
    "def rsquared(w, X, y):\n",
    "    y_pred = h(X, w)\n",
    "    return 1 - (np.sum((y- y_pred)**2))/(np.sum((y- y.mean())**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let us try gradient descent \n",
    "#### just making sure our implementation above is good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization and hyper-parameter\n",
    "w = 1e-2*np.random.random(np.shape(X_train)[1]) # weights and bias (bias is 0 in this case)\n",
    "eta = 1e-3 # step size (learning rate)\n",
    "num_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(w * X_train, axis=1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.matmul(X_train,w)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_at_eachstep = np.zeros(num_steps) # record the change of the loss function\n",
    "for i in range(num_steps):\n",
    "    loss_at_eachstep[i] = loss(w,X_train,y_train)\n",
    "    dw = gradient_loss(w,X_train,y_train)\n",
    "    w -= eta * dw\n",
    "    if i % 200 == 0:\n",
    "        print(\"loss after\", i+1, \"iterations is: \", loss(w,X_train,y_train))\n",
    "        print(\"Training R squared after\", i+1, \"iterations is: \", rsquared(w, X_train, y_train))\n",
    "        print(\"Testing R squared after\", i+1, \"iterations is: \", rsquared(w, X_test, y_test))\n",
    "    # keep track of training accuracy just making sure we are in the right direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_steps), loss_at_eachstep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "* One sample's gradient at a time.\n",
    "* Every epoch we use `np.random.permutation` to randomly permute the order of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-4 # step size (learning rate), in general SGD should have a smaller learning rate\n",
    "num_epochs = 5 # no. of outer iteration\n",
    "M = len(y_train) # no. of inner iteration\n",
    "N = M # in general you can choose M <= N\n",
    "w = 1e-2*np.random.random(np.shape(X_train)[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_loss_at_eachstep = np.zeros([num_epochs,N]) # record the change of the loss function\n",
    "# num_epochs is the # of outer iterations\n",
    "# N is the # of samples, which is the number of inner iterations\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    shuffle_index = np.random.permutation(N)\n",
    "    for m in range(M):\n",
    "        i = shuffle_index[m] # i corresponds i-th sample\n",
    "        sgd_loss_at_eachstep[e,m] = loss(w,X_train,y_train)\n",
    "        dw = gradient_loss(w,X_train[i,:],y_train[i])\n",
    "        # this is the gradient for i-th sample\n",
    "        w -= eta * dw\n",
    "        if m % 1000 ==0:\n",
    "            print(\"loss after\", e+1, \"epochs and \", m+1, \"iterations is: \", loss(w,X_train,y_train))\n",
    "            print(\"Training R squared after\", e+1, \"epochs and \", m+1, \n",
    "                  \"iterations is:\", rsquared(w, X_train, y_train))\n",
    "            print(\"Testing R squared after\", e+1, \"epochs and \", m+1, \n",
    "                  \"iterations is:\", rsquared(w, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sgd_loss_at_eachstep.reshape(-1)[:300], label=\"SGD loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading: Mini-batch SGD\n",
    "\n",
    "In the vanilla SGD, each parameter $\\mathbf{w}$ update is computed w.r.t one training sample randomly selected. In mini-batch SGD, the update is computed for a mini-batch (a small number of training samples), as opposed to a single example. The reason for this is twofold: \n",
    "* This reduces the variance in the parameter update and can lead to more stable convergence.\n",
    "* This allows the computation to be more efficient, since our code is written in a vectorized way. \n",
    "\n",
    "A typical mini-batch size is $2^k$ (32, 256, etc), although the optimal size of the mini-batch can vary for different applications, and size of dataset (e.g., AlphaGo training uses mini-batch size of 2048 positions).\n",
    "\n",
    "> Choose initial guess $\\mathbf{w}_0$, step size (learning rate) $\\eta$, <br>\n",
    "batch size $n_B$, number of inner iterations $M\\leq N/n_B$, number of epochs $n_E$ <br><br>\n",
    ">    Set $\\mathbf{w}_{M+1} = \\mathbf{w}_0$ for epoch $e=0$<br>\n",
    ">    For epoch $n=1,2, \\cdots, n_E$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w}_{0}$ for the current epoch is $\\mathbf{w}_{M+1}$ for the previous epoch.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the training samples.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{m+1} = \\mathbf{w}_m -  \\frac{\\eta}{n_B}\\sum_{i=1}^{n_B} \\nabla f_i(\\mathbf{w}; \\mathbf{x}^{(m+i)},y^{(m+i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-4 # step size (learning rate)\n",
    "num_epochs = 20\n",
    "N = len(y_train)\n",
    "num_batch = 16 # number of mini-batch\n",
    "M = int(N/num_batch)\n",
    "w = 1e-2*np.random.random(np.shape(X_train)[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdmini_loss_at_eachstep = np.zeros([num_epochs,M]) # record the change of the loss function\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    shuffle_index = np.random.permutation(N)\n",
    "    for m in range(0,N,num_batch):\n",
    "        i = shuffle_index[m:m+num_batch]\n",
    "        # indices for the gradient of loss function\n",
    "        # of the i-th sample to be evaluated \n",
    "        sgdmini_loss_at_eachstep[e,m//num_batch-1] = loss(w,X_train,y_train)\n",
    "        dw = gradient_loss(w,X_train[i,:],y_train[i])\n",
    "        w -= eta * dw\n",
    "        if m % 1000 ==0 and e % 5 == 0:\n",
    "            print(\"loss after\", e+1, \"epochs and \", m+1, \"iterations is: \", loss(w,X_train,y_train))\n",
    "            print(\"Training R squared after\", e+1, \"epochs and \", (m+1)//num_batch, \n",
    "                  \"iterations is:\", rsquared(w, X_train, y_train))\n",
    "            print(\"Testing R squared after\", e+1, \"epochs and \", (m+1)//num_batch, \n",
    "                  \"iterations is:\", rsquared(w, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "num_plot = 300\n",
    "plt.figure(figsize= [16,8])\n",
    "plt.plot(sgdmini_loss_at_eachstep.reshape(-1)[:num_plot], 'b-',\n",
    "         label=\"SGD mini-batch loss\")\n",
    "plt.plot(sgd_loss_at_eachstep.reshape(-1)[:num_plot], \n",
    "         label=\"SGD loss\", linewidth=2, color = 'green')\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
