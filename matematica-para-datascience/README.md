<p align="center"> 
  <img src="../images/foto-github.png">
</p>

<h3 align="center">Matemática para Ciencia de Datos</h3>
<p align="center">
  :bar_chart: ¡Camino a una educación autodidacta en <strong>Data Science</strong>!
  <br><br>
</p>
---------------------------------------------------------

Es la primera clase de programación dedicada en la especialización de Data Science diseñada principalmente para estudiantes de matemáticas en la Universidad de California Irvine. Se presentarán algunos de los algoritmos de facto actuales, y algunos teoremas en matemáticas detrás de la ciencia de datos / aprendizaje automático se verificarán utilizando Python, y el formato se puede adaptar a otros lenguajes populares como R y Julia.

### Prerequisites: 
**MATH 2D:** [Multivariate Calculus](https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/)
<br><br>
**MATH 3A:** [Linear Algebra (se puede tomar simultáneamente)](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/)
<br><br>
**MATH 9:** [Introduction for Numerical Analysis](https://ocw.mit.edu/courses/mathematics/18-330-introduction-to-numerical-analysis-spring-2004/)
<br><br>

#### Recomendación: 
**MATH 130A:** [Probability I](https://www.math.uci.edu/~isik/teaching/17S_MATH130A/)
<br><br>

----

| Lecture    | Contents |
|:----------|:--------|
|  Lecture 1 | [Intro to Jupyter notebooks, expressions, operations, variables](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-01-introduction.ipynb) |
|  Lecture 2 | [Defining your own functions, types (float, bool, int), Lists, IF-ELSE](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-02-Variables-and-Functions.ipynb)  |
|  Lecture 3 | [Numpy arrays I, tuples, slicing](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-03-Numpy.ipynb) |
|  Lecture 4 | [Numpy arrays II, WHILE and FOR loops vs vectorization](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-04-Loops-vs-Vectorization.ipynb) |
|  Lecture 5 | [Numpy arrays III, advanced slicing; Matplotlib I, pyplot](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-05-Matplotlib.ipynb) |
|  Lecture 6 | [Numpy arrays IV, Linear algebra routines](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-06-Linear-Algebra.ipynb) |
|  Lecture 7 | [Matplotlib II, histograms](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-07-Histogram.ipynb)|
|  Lecture 8 | [Randomness I; Matplotlib III, scatter plot](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-08-Randomness.ipynb)|
|  Lecture 9 | [Randomness II, descriptive statistics, sampling data](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-09-Statistics.ipynb)|
|  Lecture 10 | [Randomness III, random walks, Law of large numbers](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-10-Random-Walks.ipynb)|
|  Lecture 11 | [Introduction to class and methods, object-oriented programming](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-11-Class.ipynb) |
|  Lecture 12 | [Optimization I: Optimizing functions, gradient descent](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-12-Gradient-Descent.ipynb)|
|  Lecture 13 | [Fitting data I: Linear model, regression, least-square](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-13-Linear-Regression.ipynb)|
|  Lecture 14 | [Optimization II: Solving linear regression by gradient descent](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-14-Linear-Regression-II.ipynb)|
|  Lecture 15 | [Fitting data II: Overfitting, interpolation, multivariate linear regression](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-15-Overfitting.ipynb)|
|  Lecture 16 | [Classification I: Bayesian classification, supervised learning models](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-16-Classification-I.ipynb)|
|  Lecture 17 | [Classification II: Logistic regression, binary classifier](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-17-Classification-II-Binary.ipynb)|
|  Lecture 18 | [Classification III: Softmax regression, multiclass classifier](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-18-Classification-III-Multiclass.ipynb)|
|  Lecture 19 | [Optimization III: Stochastic gradient descent](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-19-Stochastic-Gradient-Descent.ipynb)|
|  Lecture 20 | [Classification IV: K-nearest neighbor](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-20-K-Nearest-Neighbor.ipynb)|
|  Lecture 21 | [Dimension reduction: Singular Value Decomposition (SVD), Principal Component Analysis (PCA)](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-21-Principal-Component-Analysis.ipynb)|
|  Lecture 22 | [Feedforward Neural Networks I: models, activation functions, regularizations](https://github.com/scaomath/UCI-Math10/blob/master/Lectures/Lecture-22-Neural-Network-I.ipynb) |
|  Lecture 23 | [Feedforward Neural Networks II: backpropagation](https://github.com/scaomath/UCI-Math10/blob/master/Lectures/Lecture-23-Neural-Network-II.ipynb) |
|  Lecture 24 | [KFold, PyTorch, Autograd, and other tools to look at](https://nbviewer.jupyter.org/github/scaomath/UCI-Math10/blob/master/Lectures/Lecture-24-Advanced-Tricks.ipynb) |

